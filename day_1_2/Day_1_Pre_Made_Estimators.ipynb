{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day_1_Pre_Made_Estimators.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/sthalles/tensorflow-tutorials/blob/master/day_1_2/Day_1_Pre_Made_Estimators.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "cqTmrQ_S2uHN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pre-Made Estimators"
      ]
    },
    {
      "metadata": {
        "id": "Rs-U0Gbg2tOT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "plt.rcParams[\"axes.grid\"] = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "09FZfGX73c10",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Estimator is a high-level Tensorflow API. \n",
        "- It is built on top of the TensorFlow core API.\n",
        "\n",
        "It follows a **train-evaluate-predict** loop.\n",
        "\n",
        "Its goal is to handle the boring steps of training an ML model like:\n",
        "\n",
        "- Creating the computational graph\n",
        "- Initializing Variables\n",
        "- Training, testing, and making predictions\n",
        "- Visualizing training specific variables (learning rate, trainable variables and performance measures)\n",
        "- Saving the model\n",
        "\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/tensorflow_programming_environment.png)\n",
        "\n",
        "To write a TensorFlow program based on pre-made Estimators, you must perform the following tasks:\n",
        "\n",
        "1.  Create one or more **input functions**.\n",
        "2.  Define the model's **feature columns**.\n",
        "3. Instantiate an Estimator, specifying the feature columns and various hyperparameters.\n",
        "4. Train, Evaluate and Test\n",
        "\n",
        "# Loading Data"
      ]
    },
    {
      "metadata": {
        "id": "zyFPmARd8fUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"DO NOT NEED CHANGES\"\"\"\n",
        "def maybe_download():\n",
        "    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)\n",
        "    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\n",
        "\n",
        "    return train_path, test_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GmASDzpA3zBn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"DO NOT NEED CHANGES\"\"\"\n",
        "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
        "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
        "\n",
        "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
        "                    'PetalLength', 'PetalWidth', 'Species']\n",
        "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "  train_path, test_path = maybe_download()\n",
        "  train_data = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
        "  test_data = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
        "  \n",
        "  train_input = train_data[CSV_COLUMN_NAMES[0:-1]]\n",
        "  train_labels = train_data[CSV_COLUMN_NAMES[-1]]\n",
        "  \n",
        "  test_input = test_data[CSV_COLUMN_NAMES[0:-1]]\n",
        "  test_labels = test_data[CSV_COLUMN_NAMES[-1]]\n",
        "  \n",
        "  return (train_input, train_labels), (test_input, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Emtq_oW0HtbZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://www.tensorflow.org/images/iris_three_species.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "_HNxFcVM8fyz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = load_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4dHRFlWUgDU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "e67724a4-0d2f-460c-ca13-a9ef693a5989"
      },
      "cell_type": "code",
      "source": [
        "print(\"Visualize the Dataset shapes\")\n",
        "print(\"Train input:\", X_train.shape)\n",
        "print(\"Train labels:\", y_train.shape)\n",
        "print(\"Test input:\", X_test.shape)\n",
        "print(\"Test labels:\", y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Visualize the Dataset shapes\n",
            "Train input: (120, 4)\n",
            "Train labels: (120,)\n",
            "Test input: (30, 4)\n",
            "Test labels: (30,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3nUjVLf_H1nh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Iris data set contains 4 features and 1 label. \n",
        "\n",
        "The 4 **features** identify the following botanical characteristics of individual Iris flowers:\n",
        "\n",
        "- sepal length\n",
        "- sepal width\n",
        "- petal length\n",
        "- petal width\n",
        "\n",
        "![alt text](http://s5047.pcdn.co/wp-content/uploads/2015/04/iris_petal_sepal.png)\n",
        "\n",
        "Our model will represent these features as float32 numerical data.\n",
        "\n",
        "The **label** identifies the Iris species, which must be one of the following:\n",
        "\n",
        "- Iris setosa (0)\n",
        "- Iris versicolor (1)\n",
        "- Iris virginica (2)\n",
        "\n",
        "Our model will represent the label as categorical data of type int."
      ]
    },
    {
      "metadata": {
        "id": "t3GLxIFMHU0E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "c43d2049-ac71-45dc-b6d3-1ac38b6b1055"
      },
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SepalLength</th>\n",
              "      <th>SepalWidth</th>\n",
              "      <th>PetalLength</th>\n",
              "      <th>PetalWidth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.4</td>\n",
              "      <td>2.8</td>\n",
              "      <td>5.6</td>\n",
              "      <td>2.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>3.3</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.9</td>\n",
              "      <td>2.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.7</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SepalLength  SepalWidth  PetalLength  PetalWidth\n",
              "0          6.4         2.8          5.6         2.2\n",
              "1          5.0         2.3          3.3         1.0\n",
              "2          4.9         2.5          4.5         1.7\n",
              "3          4.9         3.1          1.5         0.1\n",
              "4          5.7         3.8          1.7         0.3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "id": "G8Iuj0njHaNX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "ea0ac7c0-1938-4cdc-bed2-eefa10970b3e"
      },
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2\n",
              "1    1\n",
              "2    2\n",
              "3    0\n",
              "4    0\n",
              "Name: Species, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "metadata": {
        "id": "IFad94RrSCyZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Exercise: Creating Validation (Dev) set\n",
        "\n",
        "The Iris dataset is divided into Training and Testing sets.\n",
        "\n",
        "This time, let's **NOT** use the Test set for development. \n",
        "\n",
        "The idea is to have a different set for which we can perform **hyperparameter tuning** during training.\n",
        "\n",
        "That is the Dev or **Validation set**.\n",
        "\n",
        "- Separate the training set into 2 sets: **training** and **validation**.\n",
        "\n",
        "### Tip\n",
        "- *The size of the validation set may vary. 15 - 20% are good choices for this dataset.*\n",
        "- Use sklearn [train_test_split()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n"
      ]
    },
    {
      "metadata": {
        "id": "PzlFnFP7SBcL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CODE GOES HERE\n",
        "X_train, X_val, y_train, y_val = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OYQDKxHFVAyY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "4abce849-608e-48d7-fee1-cd0b9c925bbe"
      },
      "cell_type": "code",
      "source": [
        "print(\"Training/validation shapes\")\n",
        "print(\"Train input:\", X_train.shape)\n",
        "print(\"Train labels:\", y_train.shape)\n",
        "print(\"Test input:\", X_val.shape)\n",
        "print(\"Test labels:\", y_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training/validation shapes\n",
            "Train input: (102, 4)\n",
            "Train labels: (102,)\n",
            "Test input: (18, 4)\n",
            "Test labels: (18,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XE0JRtOAJ1Qi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creating Input funcions\n",
        "\n",
        "**Input functions are responsible for feeding data to ML Models created with the TF Estimators API.**\n",
        "\n",
        "The Estimator expects an *input_function()* to return a [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) or a tuple of *(features, labels)*.\n",
        "\n",
        "Let's use the **tf.data.Dataset** as our input streaming.\n",
        "-  [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) is the recommended Tensorflow input pipeline.\n",
        "\n",
        "Hint: [tf.data.Dataset template](https://www.tensorflow.org/get_started/datasets_quickstart)\n",
        "\n",
        "## Exercise\n",
        "\n",
        "Go ahaed and complete the input functions for training and validation."
      ]
    },
    {
      "metadata": {
        "id": "g69sCY5S8zYk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_input_fn(features, labels, batch_size):\n",
        "  \"\"\"\n",
        "  Provides input data for training\n",
        "  Return: A 'tf.data.Dataset' object: tuple (features, labels). \n",
        "          Or tuple (features, labels)\n",
        "  \"\"\"\n",
        "  \n",
        "  # Create the dataset object and return it\n",
        "  \n",
        "  dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "  \n",
        "  # Make sure you shuffle and define the batch size as given by the 'batch_size' parameter\n",
        "  # CODE GOES HERE\n",
        "  \n",
        "  return dataset # return the dataset object"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3aWlJkPaGrXg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval_input_fn(features, labels, batch_size):\n",
        "  \n",
        "  features = dict(features)\n",
        "  if labels is None:\n",
        "    inputs = features\n",
        "  else:\n",
        "    inputs = (dict(features), labels)\n",
        "  \n",
        "  # Create the tf.data.Dataset() object and return it\n",
        "  # Set the number of epochs to 1, define the batch size.\n",
        "  # Do we need to shuffle the data??\n",
        "  # CODE GOES HERE\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pf6z_QJxEFFK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Defining the Feature Columns\n",
        "\n",
        "Now, we have to convert our data into **Tensors** so that our Model can use it.\n",
        "\n",
        "Feature Columns provides a representation of how our model should interpret the data it will receive.\n",
        "- Is this column Numerical, Categorical or what?\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/feature_columns/some_constructors.jpg)\n",
        "\n",
        "**Feature Columns** define the **type of features** we are going to feed into our Models.\n",
        "\n",
        "The choice of Feature Columns depends on the Variable and Model type.\n",
        "\n",
        "Since the 4 feature of the Iris dataset are represented as continuos values, we need to especify it when creating the feature columns.\n",
        "\n",
        "To do that, we use the: [tf.feature_column.numeric_column()](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column) constructor.\n",
        "\n",
        "  - **tf.feature_column.numeric_column()** Represents real valued or numerical features.\n",
        "  \n",
        "Info++: [Feature Engineering](https://www.tensorflow.org/get_started/feature_columns)\n",
        "  \n",
        " \n"
      ]
    },
    {
      "metadata": {
        "id": "ZHeDQW56Li-s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e61289e2-d2f2-4e78-aae4-3cf891e18aa1"
      },
      "cell_type": "code",
      "source": [
        "X_train.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "metadata": {
        "id": "PJEWtWQ_-v9s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# feature columns define how to use the feature data\n",
        "my_feature_columns = []\n",
        "\n",
        "# tf.feature_column.numeric_column(key='<COLUMN_NAME>')\n",
        "\n",
        "# CODE GOES HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iihk9U-ubQbr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "1. Tune the hyperparameters bellow."
      ]
    },
    {
      "metadata": {
        "id": "QRwjJ4qbbbzR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate =\n",
        "number_of_classes = # number of classes from the Iris dataset\n",
        "batch_size = # number of examples to feed to the network per step\n",
        "max_step = # maximum number of training steps\n",
        "regularization = # regularization strength\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wCQTOJnbCjFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the Estimator\n",
        "\n",
        "An Estimator encapsulates all the necessary parts of a model. \n",
        "\n",
        "Some of the available Estimators include:\n",
        "\n",
        "1. **BoostedTrees** Classifier/Regressor\n",
        "2. **DNN Classifier**/Regressor\n",
        "3. **DNNLinearCombined** Classifier/Regressor\n",
        "4. **Linear** Classifier/Regressor\n",
        "\n",
        "Checkout: [tf.estimator](https://www.tensorflow.org/api_docs/python/tf/estimator)\n",
        "\n",
        "## Exercise\n",
        "\n",
        "1. Use the [tf.estimator.LinearClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier) to classify the Iris Dataset.\n",
        "\n",
        "Things to keep in mind.\n",
        "  - Linear models are very simple, for this case, pay special attention to the **number of classes** and the **learning rate** tunning.\n",
        "  - Play with different configurations of **batch size**, it can have dramatic effects on how quick the model converges."
      ]
    },
    {
      "metadata": {
        "id": "jQBtZvi8CHKf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CODE GOES HERE\n",
        "classifier = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aGnEdrbmoBGa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tip\n",
        "\n",
        "You do NOT need to define nor control the TensorFlow Session. \n",
        "*Estimators take care of that for you.*\n",
        "\n",
        "## Exercise\n",
        "\n",
        "Use the *classifier.train()* method to train the built classifier.\n",
        "- Pass the  *train_input_fn()* as a lambda so you can control its input parameters.\n",
        "- Set the *steps* parameter to the maximum number of steps you defined to train your model."
      ]
    },
    {
      "metadata": {
        "id": "L4U3QtsFDHpB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier.train(input_fn=, \n",
        "                 steps= #  Number of steps for which to train model.\n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-uMr-6AppGzo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise - Validation\n",
        "\n",
        "Use the *eval_input_fn()* method to measure the performance of your model.\n",
        "\n",
        "- Use the **Validation set** to evaluate how good your model is becoming. \n",
        "- Here you are allowed to use this dataset to **change the model's hyperparameters**.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hA3DUaUIDtFY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_results = classifier.evaluate(input_fn=...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "InvMPSwxo7_F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise -Test\n",
        "\n",
        "Now, we want to use our **Testing set** to see how good our model really is.\n",
        "- Use the Unseen Test set now.\n",
        "- Use the same *eval_input_fn()* method you used for validation."
      ]
    },
    {
      "metadata": {
        "id": "-l-9hbiRmGln",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = classifier.predict(input_fn=...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zaU7Jzxbmf7X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Visualize the predictions and confidence for each of the Test set records\n",
        "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
        "template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
        "prediction_values = []\n",
        "for pred_dict, expec in zip(predictions, y_test):\n",
        "  \n",
        "  # get the prediction class for each instance and save it\n",
        "  class_id = pred_dict['class_ids'][0]\n",
        "  prediction_values.append(class_id)\n",
        "  \n",
        "  # get and display the probabilities for each instance classification\n",
        "  probability = pred_dict['probabilities'][class_id]\n",
        "\n",
        "  print(template.format(SPECIES[class_id],\n",
        "                    100 * probability, SPECIES[expec]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WpMfuMoSq8Gq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o6-NuIOioMHJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(predictions)\n",
        "cnf_matrix = confusion_matrix(y_test, prediction_values)\n",
        "plot_confusion_matrix(cnf_matrix, classes=SPECIES,\n",
        "                      title='Confusion matrix, without normalization')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AuxmT16LtogI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Networks\n",
        "\n",
        "### Definition:\n",
        "\n",
        "\"*...a computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs.*\"\n",
        "\n",
        "- Function approximator\n",
        "- Very powerful for non-linear relationships\n",
        "- Represents a function as the composition of many functions.\n",
        "\n",
        "## Exercise\n",
        "\n",
        "1. Change the Linear Model Estimator to a Deep Neural Network.\n",
        "- Head over to the Tensorflow documentation for [tf.estimator.DNNClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) and check it out.\n",
        "\n",
        "Think about:\n",
        "\n",
        "- How many layers you need\n",
        "- The number of units in each layer\n",
        "- The activation function used by default\n",
        "- The Gradient Descent Optimizer. \n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/custom_estimators/full_network.png)\n",
        "\n",
        "## Architecturing your network\n",
        "\n",
        "![LeNet-5](http://cs231n.github.io/assets/nn1/layer_sizes.jpeg)\n",
        "\n",
        "Neural Nets with more hidden layers are able to represent more complex functions. \n",
        "\n",
        "- With more power comes complicated decision boundaries.\n",
        "\n",
        "Take care with **Overfitting**!\n",
        "\n",
        "- It occurs when a model with **high capacity** fits the noise in the data instead of the (assumed) underlying relationship.\n"
      ]
    },
    {
      "metadata": {
        "id": "07xM_Mh0dlc8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Regularization\n",
        "\n",
        "Effects of Regularization. The figure below shows the decision boundaries of the same DNN (20 hidden units), with different regularization penalties. \n",
        "\n",
        "Note that more regularization smooths the decision boundary.\n",
        "\n",
        "- It fights **Overfitting**.\n",
        "\n",
        "![alt text](http://cs231n.github.io/assets/nn1/reg_strengths.jpeg)"
      ]
    },
    {
      "metadata": {
        "id": "6C6iw34VuR2Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
